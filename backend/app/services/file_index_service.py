"""
æ–‡ä»¶ç´¢å¼•æœåŠ¡

æ•´åˆæ–‡ä»¶æ‰«æã€å…ƒæ•°æ®æå–ã€å†…å®¹è§£æå’Œç´¢å¼•æ„å»ºåŠŸèƒ½çš„ä¸»è¦æœåŠ¡ç±»ã€‚
"""

import os
import uuid
import asyncio
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime

# å¯¼å…¥è‡ªå®šä¹‰æœåŠ¡
from .file_scanner import FileScanner, FileInfo
from .metadata_extractor import MetadataExtractor
from .content_parser import ContentParser, ParsedContent
from .chunk_index_service import get_chunk_index_service
from app.core.logging_config import logger

# å¯¼å…¥ç»Ÿä¸€é…ç½®
from app.core.config import get_settings
settings = get_settings()


class FileIndexService:
    """æ–‡ä»¶ç´¢å¼•æœåŠ¡

    æ ¸å¿ƒåŠŸèƒ½ï¼š
    - ç»Ÿä¸€çš„ç´¢å¼•ç®¡ç†æ¥å£
    - ç´¢å¼•ä»»åŠ¡è°ƒåº¦å’Œç›‘æ§
    - å¢é‡ç´¢å¼•æ›´æ–°
    - é”™è¯¯å¤„ç†å’Œæ¢å¤æœºåˆ¶
    - æ€§èƒ½ä¼˜åŒ–ï¼ˆå†…å­˜ç®¡ç†ã€æ‰¹å¤„ç†ï¼‰
    """

    def __init__(
        self,
        data_root: str,
        faiss_index_path: Optional[str] = None,
        whoosh_index_path: Optional[str] = None,
        use_chinese_analyzer: bool = True,
        scanner_config: Optional[Dict[str, Any]] = None,
        parser_config: Optional[Dict[str, Any]] = None
    ):
        """åˆå§‹åŒ–æ–‡ä»¶ç´¢å¼•æœåŠ¡

        Args:
            data_root: æ•°æ®æ ¹ç›®å½•
            faiss_index_path: Faissç´¢å¼•æ–‡ä»¶è·¯å¾„
            whoosh_index_path: Whooshç´¢å¼•ç›®å½•è·¯å¾„
            use_chinese_analyzer: æ˜¯å¦ä½¿ç”¨ä¸­æ–‡åˆ†æå™¨
            scanner_config: æ–‡ä»¶æ‰«æå™¨é…ç½®
            parser_config: å†…å®¹è§£æå™¨é…ç½®
        """
        self.data_root = Path(data_root)
        self.data_root.mkdir(parents=True, exist_ok=True)

        # è®¾ç½®ç´¢å¼•è·¯å¾„
        if not faiss_index_path:
            faiss_index_path = str(self.data_root / "indexes" / "faiss" / "document_index.faiss")
        if not whoosh_index_path:
            whoosh_index_path = str(self.data_root / "indexes" / "whoosh")

        # åˆå§‹åŒ–å­æœåŠ¡
        self.scanner = FileScanner(**(scanner_config or {}))
        self.metadata_extractor = MetadataExtractor()
        self.content_parser = ContentParser(**(parser_config or {}))

        # å­˜å‚¨ä¼ ç»Ÿç´¢å¼•è·¯å¾„ï¼ˆç”¨äºå…¼å®¹æ€§ï¼Œä½†ä¸»è¦ä½¿ç”¨åˆ†å—ç´¢å¼•ï¼‰
        self.traditional_faiss_path = faiss_index_path
        self.traditional_whoosh_path = whoosh_index_path

        # ç´¢å¼•çŠ¶æ€
        self.index_status = {
            'is_building': False,
            'last_index_time': None,
            'total_files_indexed': 0,
            'failed_files': 0,
            'indexing_progress': 0.0
        }

        # å†…å­˜ä¸­ç¼“å­˜å·²ç´¢å¼•æ–‡ä»¶ä¿¡æ¯ï¼ˆç”¨äºå˜æ›´æ£€æµ‹ï¼‰
        self._indexed_files_cache: Dict[str, FileInfo] = {}

    def _should_be_chunked(self, content_length: int) -> bool:
        """åˆ¤æ–­æ–‡ä»¶æ˜¯å¦åº”è¯¥è¢«åˆ†å—å¤„ç†

        Args:
            content_length: æ–‡ä»¶å†…å®¹çš„å­—ç¬¦é•¿åº¦

        Returns:
            bool: æ˜¯å¦åº”è¯¥åˆ†å—
        """
        # è·å–åˆ†å—é…ç½®
        from app.core.config import get_settings
        settings = get_settings()
        chunk_size = settings.chunk.default_chunk_size

        # å¦‚æœå†…å®¹é•¿åº¦å¤§äºåˆ†å—å¤§å°ï¼Œåˆ™è¿›è¡Œåˆ†å—
        return content_length > chunk_size

    async def load_indexed_files_cache(self):
        """ä»æ•°æ®åº“åŠ è½½å·²ç´¢å¼•æ–‡ä»¶åˆ°ç¼“å­˜ï¼ˆå¯åŠ¨æ—¶è°ƒç”¨ï¼‰"""
        try:
            from app.core.database import SessionLocal
            from app.models.file import FileModel

            logger.info("å¼€å§‹ä»æ•°æ®åº“åŠ è½½ç´¢å¼•ç¼“å­˜...")
            start_time = datetime.now()

            db = SessionLocal()
            try:
                # æŸ¥è¯¢æ‰€æœ‰å·²ç´¢å¼•çš„æ–‡ä»¶
                indexed_files = db.query(FileModel).filter(
                    FileModel.is_indexed == True,
                    FileModel.index_status == 'completed'
                ).all()

                loaded_count = 0
                for file_record in indexed_files:
                    try:
                        # è½¬æ¢æ•°æ®åº“è®°å½•ä¸ºFileInfoå¯¹è±¡
                        file_info = FileInfo(
                            path=file_record.file_path,
                            name=file_record.file_name,
                            extension=file_record.file_extension,
                            size=file_record.file_size,
                            created_time=file_record.created_at,
                            modified_time=file_record.modified_at,
                            mime_type=file_record.mime_type or "application/octet-stream",
                            content_hash=file_record.content_hash or ""
                        )

                        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä»ç„¶å­˜åœ¨ä¸”æœªè¢«ä¿®æ”¹
                        if os.path.exists(file_record.file_path):
                            current_stat = os.stat(file_record.file_path)
                            current_modified = datetime.fromtimestamp(current_stat.st_mtime)

                            # åªæœ‰å½“æ–‡ä»¶æœªè¢«ä¿®æ”¹æ—¶æ‰åŠ å…¥ç¼“å­˜
                            if current_modified <= file_record.modified_at:
                                self._indexed_files_cache[file_info.path] = file_info
                                loaded_count += 1
                                logger.debug(f"âœ… åŠ è½½åˆ°ç¼“å­˜: {file_record.file_name}")
                            else:
                                logger.debug(f"âŒ æ–‡ä»¶å·²ä¿®æ”¹ï¼Œè·³è¿‡: {file_record.file_name}")
                        else:
                            logger.debug(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {file_record.file_path}")

                    except Exception as e:
                        logger.warning(f"åŠ è½½æ–‡ä»¶åˆ°ç¼“å­˜å¤±è´¥ {file_record.file_path}: {e}")
                        continue

                duration = datetime.now() - start_time
                logger.info(f"ç´¢å¼•ç¼“å­˜åŠ è½½å®Œæˆ: {loaded_count}/{len(indexed_files)} ä¸ªæ–‡ä»¶ï¼Œè€—æ—¶ {duration.total_seconds():.2f} ç§’")

                # å¦‚æœç¼“å­˜çš„æ–‡ä»¶æ•°é‡è¿œå°‘äºæ•°æ®åº“è®°å½•ï¼Œå¯èƒ½éœ€è¦é‡å»ºç´¢å¼•
                if loaded_count < len(indexed_files) * 0.8:  # å°‘äº80%
                    logger.warning(f"ç¼“å­˜å®Œæ•´æ€§è¾ƒä½ ({loaded_count}/{len(indexed_files)})ï¼Œå»ºè®®æ£€æŸ¥ç´¢å¼•çŠ¶æ€")

            except Exception as e:
                logger.error(f"ä»æ•°æ®åº“åŠ è½½ç´¢å¼•ç¼“å­˜å¤±è´¥: {e}")
            finally:
                db.close()

        except Exception as e:
            logger.error(f"åŠ è½½ç´¢å¼•ç¼“å­˜æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")

    async def build_full_index(
        self,
        scan_paths: List[str],
        progress_callback: Optional[callable] = None
    ) -> Dict[str, Any]:
        """æ„å»ºå®Œæ•´ç´¢å¼•

        Args:
            scan_paths: è¦æ‰«æçš„è·¯å¾„åˆ—è¡¨
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°

        Returns:
            Dict[str, Any]: æ„å»ºç»“æœ
        """
        if self.index_status['is_building']:
            return {
                'success': False,
                'error': 'ç´¢å¼•æ­£åœ¨æ„å»ºä¸­ï¼Œè¯·ç­‰å¾…å®Œæˆåå†è¯•'
            }

        self.index_status['is_building'] = True
        self.index_status['indexing_progress'] = 0.0

        try:
            logger.info(f"å¼€å§‹æ„å»ºå®Œæ•´ç´¢å¼•ï¼Œæ‰«æè·¯å¾„: {scan_paths}")
            start_time = datetime.now()

            # 1. æ‰«ææ‰€æœ‰æ–‡ä»¶
            all_files = []
            for path in scan_paths:
                files = self.scanner.scan_directory(
                    path,
                    recursive=True,
                    include_hidden=False,
                    progress_callback=lambda current, total: self._update_progress(
                        current, total, progress_callback, stage="æ‰«ææ–‡ä»¶"
                    )
                )
                all_files.extend(files)

            if not all_files:
                return {
                    'success': False,
                    'error': 'æ²¡æœ‰æ‰¾åˆ°æ”¯æŒçš„æ–‡ä»¶'
                }

            # 2. å¤„ç†æ–‡ä»¶å¹¶æ„å»ºæ–‡æ¡£
            documents = []
            failed_count = 0

            for i, file_info in enumerate(all_files):
                try:
                    doc = await self._process_file_to_document(file_info)
                    if doc:
                        documents.append(doc)

                    # æ›´æ–°è¿›åº¦
                    self.index_status['indexing_progress'] = 30.0 + (i / len(all_files)) * 50.0
                    if progress_callback:
                        progress_callback(f"å¤„ç†æ–‡ä»¶: {file_info.name}",
                                         self.index_status['indexing_progress'])

                except Exception as e:
                    logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥ {file_info.path}: {e}")
                    failed_count += 1

            if not documents:
                return {
                    'success': False,
                    'error': 'æ²¡æœ‰æˆåŠŸå¤„ç†çš„æ–‡æ¡£'
                }

            # 3. å…ˆä¿å­˜æ–‡ä»¶æ•°æ®åˆ°æ•°æ®åº“ï¼ˆç¡®ä¿åˆ†å—æœåŠ¡èƒ½æ‰¾åˆ°æ–‡ä»¶è®°å½•ï¼‰
            self.index_status['indexing_progress'] = 70.0
            if progress_callback:
                progress_callback("ä¿å­˜æ–‡ä»¶åˆ°æ•°æ®åº“", 70.0)

            logger.info(f"å¼€å§‹ä¿å­˜ {len(documents)} ä¸ªæ–‡ä»¶åˆ°æ•°æ®åº“")
            await self._save_files_to_database(all_files, documents)
            logger.info("æ–‡ä»¶ä¿å­˜åˆ°æ•°æ®åº“å®Œæˆ")

            # 4. æ„å»ºç´¢å¼•ï¼ˆä¸»è¦ä½¿ç”¨åˆ†å—ç´¢å¼•ï¼‰
            self.index_status['indexing_progress'] = 80.0
            if progress_callback:
                progress_callback("æ„å»ºç´¢å¼•", 80.0)

            chunk_index_success = False
            index_success = False
            index_error = None  # ç”¨äºè®°å½•è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯

            try:
                if progress_callback:
                    progress_callback("æ„å»ºåˆ†å—ç´¢å¼•", 85.0)

                logger.info("å¼€å§‹æ„å»ºåˆ†å—ç´¢å¼•")
                chunk_index_service = get_chunk_index_service()
                logger.info("åˆ†å—ç´¢å¼•æœåŠ¡è·å–æˆåŠŸ")

                chunk_index_success = await chunk_index_service.build_chunk_indexes(documents)
                logger.info(f"åˆ†å—ç´¢å¼•æ„å»ºå®Œæˆï¼Œç»“æœ: {chunk_index_success}")

                if chunk_index_success:
                    logger.info("åˆ†å—ç´¢å¼•æ„å»ºæˆåŠŸ")
                    index_success = True  # åˆ†å—ç´¢å¼•æˆåŠŸä»£è¡¨æ•´ä½“æˆåŠŸ
                else:
                    logger.warning("åˆ†å—ç´¢å¼•æ„å»ºå¤±è´¥")
                    index_error = "åˆ†å—ç´¢å¼•æ„å»ºå¤±è´¥ï¼šFaisså‘é‡ç´¢å¼•æˆ–Whooshå…¨æ–‡ç´¢å¼•æ„å»ºå¤±è´¥"

            except Exception as e:
                logger.error(f"æ„å»ºåˆ†å—ç´¢å¼•å¤±è´¥: {e}")
                import traceback
                logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
                chunk_index_success = False
                index_error = f"åˆ†å—ç´¢å¼•æ„å»ºå¼‚å¸¸ï¼š{str(e)}"

            # 5. æ›´æ–°ç¼“å­˜å’ŒçŠ¶æ€
            if index_success:
                self._update_indexed_files_cache(all_files)
                self.index_status.update({
                    'last_index_time': datetime.now(),
                    'total_files_indexed': len(documents),
                    'failed_files': failed_count,
                    'indexing_progress': 100.0
                })

            duration = datetime.now() - start_time
            logger.info(f"å®Œæ•´ç´¢å¼•æ„å»ºå®Œæˆï¼Œè€—æ—¶: {duration.total_seconds():.2f} ç§’")

            # è·å–åˆ†å—ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯
            chunk_index_stats = {}
            if chunk_index_success:
                try:
                    chunk_index_service = get_chunk_index_service()
                    chunk_index_stats = chunk_index_service.get_index_stats()
                except Exception as e:
                    logger.warning(f"è·å–åˆ†å—ç´¢å¼•ç»Ÿè®¡å¤±è´¥: {e}")

            return {
                'success': index_success,
                'total_files_found': len(all_files),
                'documents_indexed': len(documents),
                'failed_files': failed_count,
                'duration_seconds': duration.total_seconds(),
                'chunk_index_stats': chunk_index_stats,
                'chunk_index_success': chunk_index_success,
                'error': index_error  # æ·»åŠ è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            }

        except Exception as e:
            logger.error(f"æ„å»ºå®Œæ•´ç´¢å¼•å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e)
            }
        finally:
            self.index_status['is_building'] = False

    def _build_full_index_sync(self, scan_paths: List[str]) -> Dict[str, Any]:
        """åŒæ­¥ç‰ˆæœ¬çš„å®Œæ•´ç´¢å¼•æ„å»º

        Args:
            scan_paths: è¦æ‰«æçš„è·¯å¾„åˆ—è¡¨

        Returns:
            Dict[str, Any]: æ„å»ºç»“æœ
        """
        import asyncio

        try:
            # åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯æ¥è¿è¡Œå¼‚æ­¥å‡½æ•°
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                return loop.run_until_complete(self.build_full_index(scan_paths))
            finally:
                loop.close()
        except Exception as e:
            logger.error(f"åŒæ­¥æ„å»ºå®Œæ•´ç´¢å¼•å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e)
            }

    async def update_incremental_index(
        self,
        scan_paths: List[str]
    ) -> Dict[str, Any]:
        """å¢é‡æ›´æ–°ç´¢å¼•

        Args:
            scan_paths: è¦æ‰«æçš„è·¯å¾„åˆ—è¡¨

        Returns:
            Dict[str, Any]: æ›´æ–°ç»“æœ
        """
        try:
            logger.info(f"å¼€å§‹å¢é‡æ›´æ–°ç´¢å¼•ï¼Œæ‰«æè·¯å¾„: {scan_paths}")
            start_time = datetime.now()

            chunk_index_service = get_chunk_index_service()
            if not chunk_index_service._chunk_indexes_exist():
                logger.info("ç´¢å¼•ä¸å­˜åœ¨ï¼Œæ‰§è¡Œå®Œæ•´ç´¢å¼•æ„å»º")
                # ç›´æ¥åŒæ­¥è°ƒç”¨ï¼Œå› ä¸ºbuild_full_indexä¼šå¤„ç†å¼‚æ­¥æ“ä½œ
                return self._build_full_index_sync(scan_paths)

            # 1. æ‰«ææ–‡ä»¶å˜æ›´
            all_changes = []
            all_deletions = []

            # ğŸ” è°ƒè¯•ï¼šæ£€æŸ¥ç¼“å­˜çŠ¶æ€
            logger.info(f"ğŸ” å¢é‡æ›´æ–°è°ƒè¯•ï¼šå½“å‰ç¼“å­˜ä¸­æœ‰ {len(self._indexed_files_cache)} ä¸ªæ–‡ä»¶")
            if len(self._indexed_files_cache) == 0:
                logger.warning("âš ï¸ ç¼“å­˜ä¸ºç©ºï¼è¿™å°†å¯¼è‡´å…¨é‡é‡å»ºï¼")
                # åˆ—å‡ºç¼“å­˜ä¸­çš„å‡ ä¸ªæ–‡ä»¶è·¯å¾„ç”¨äºè°ƒè¯•
                for i, (path, file_info) in enumerate(list(self._indexed_files_cache.items())[:3]):
                    logger.debug(f"ç¼“å­˜æ–‡ä»¶ {i+1}: {path}")
            else:
                logger.info("âœ… ç¼“å­˜æœ‰æ•°æ®ï¼Œè¿›è¡Œå¢é‡æ›´æ–°")

            for path in scan_paths:
                logger.info(f"ğŸ” æ‰«æè·¯å¾„å˜æ›´: {path}")
                changed_files, deleted_files, _ = self.scanner.scan_changes(
                    path,
                    self._indexed_files_cache,
                    recursive=True,
                    include_hidden=False
                )
                logger.info(f"ğŸ” æ‰«æç»“æœ: å˜æ›´æ–‡ä»¶ {len(changed_files)} ä¸ª, åˆ é™¤æ–‡ä»¶ {len(deleted_files)} ä¸ª")
                if changed_files:
                    for file_info in changed_files[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                        logger.debug(f"  å˜æ›´: {file_info.path}")
                all_changes.extend(changed_files)
                all_deletions.extend(deleted_files)

            if not all_changes and not all_deletions:
                return {
                    'success': True,
                    'message': 'æ²¡æœ‰æ–‡ä»¶å˜æ›´',
                    'changed_files': 0,
                    'deleted_files': 0
                }

            # 2. å¤„ç†å˜æ›´çš„æ–‡ä»¶
            new_documents = []
            for file_info in all_changes:
                try:
                    doc = await self._process_file_to_document(file_info)
                    if doc:
                        new_documents.append(doc)
                except Exception as e:
                    logger.error(f"å¤„ç†å˜æ›´æ–‡ä»¶å¤±è´¥ {file_info.path}: {e}")

            # 3. ä¿å­˜å˜æ›´æ–‡ä»¶åˆ°æ•°æ®åº“ï¼ˆç¡®ä¿è·å¾—æ­£ç¡®çš„æ•´æ•°IDï¼‰
            if new_documents:
                logger.info(f"å¼€å§‹ä¿å­˜ {len(new_documents)} ä¸ªå˜æ›´æ–‡ä»¶åˆ°æ•°æ®åº“")
                await self._save_files_to_database(all_changes, new_documents)
                logger.info("å˜æ›´æ–‡ä»¶ä¿å­˜åˆ°æ•°æ®åº“å®Œæˆ")

            # 4. ä»ç´¢å¼•ä¸­åˆ é™¤å·²åˆ é™¤çš„æ–‡ä»¶
            deleted_count = 0
            if all_deletions:
                logger.info(f"å¼€å§‹åˆ é™¤ {len(all_deletions)} ä¸ªå·²åˆ é™¤æ–‡ä»¶çš„ç´¢å¼•")
                for deleted_file_path in all_deletions:
                    try:
                        # ä½¿ç”¨åˆ†å—ç´¢å¼•æœåŠ¡åˆ é™¤æ–‡ä»¶
                        delete_result = await chunk_index_service.delete_file_from_indexes(deleted_file_path)
                        if delete_result.get('success', False):
                            deleted_count += delete_result.get('deleted_chunks', 0)
                            logger.info(f"æˆåŠŸåˆ é™¤æ–‡ä»¶ç´¢å¼•: {deleted_file_path}")
                        else:
                            logger.error(f"åˆ é™¤æ–‡ä»¶ç´¢å¼•å¤±è´¥: {deleted_file_path}, é”™è¯¯: {delete_result.get('error')}")
                    except Exception as e:
                        logger.error(f"åˆ é™¤æ–‡ä»¶ç´¢å¼•å¼‚å¸¸: {deleted_file_path}, é”™è¯¯: {e}")

                logger.info(f"æ–‡ä»¶ç´¢å¼•åˆ é™¤å®Œæˆï¼Œæ€»å…±åˆ é™¤äº† {deleted_count} ä¸ªåˆ†å—")

            # 5. æ·»åŠ æ–°æ–‡æ¡£åˆ°ç´¢å¼•
            if new_documents:
                chunk_index_success = await chunk_index_service.build_chunk_indexes(new_documents)
                if not chunk_index_success:
                    logger.warning("å¢é‡æ›´æ–°ä¸­æ„å»ºåˆ†å—ç´¢å¼•å¤±è´¥ï¼Œä½†ç»§ç»­å¤„ç†")

            # 6. æ›´æ–°ç¼“å­˜
            # ä»ç¼“å­˜ä¸­ç§»é™¤å·²åˆ é™¤çš„æ–‡ä»¶
            for deleted_path in all_deletions:
                self._indexed_files_cache.pop(deleted_path, None)

            # æ›´æ–°ç¼“å­˜ä¸­çš„å˜æ›´æ–‡ä»¶
            for file_info in all_changes:
                self._indexed_files_cache[file_info.path] = file_info

            duration = datetime.now() - start_time
            logger.info(f"å¢é‡ç´¢å¼•æ›´æ–°å®Œæˆï¼Œè€—æ—¶: {duration.total_seconds():.2f} ç§’")

            return {
                'success': True,
                'changed_files': len(all_changes),
                'deleted_files': len(all_deletions),
                'new_documents': len(new_documents),
                'duration_seconds': duration.total_seconds(),
                'index_stats': chunk_index_service.get_index_stats()
            }

        except Exception as e:
            logger.error(f"å¢é‡æ›´æ–°ç´¢å¼•å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e)
            }

    async def _save_files_to_database(self, all_files: List[FileInfo], documents: List[Dict[str, Any]]):
        """ä¿å­˜æ–‡ä»¶æ•°æ®åˆ°æ•°æ®åº“

        Args:
            all_files: æ‰«æåˆ°çš„æ‰€æœ‰æ–‡ä»¶åˆ—è¡¨
            documents: å¤„ç†æˆåŠŸçš„æ–‡æ¡£åˆ—è¡¨
        """
        try:
            from app.core.database import SessionLocal
            from app.models.file import FileModel
            from app.models.file_content import FileContentModel
            import hashlib

            db = SessionLocal()
            try:
                logger.info(f"å¼€å§‹ä¿å­˜ {len(documents)} ä¸ªæ–‡ä»¶åˆ°æ•°æ®åº“")

                for i, (file_info, document) in enumerate(zip(all_files, documents)):
                    try:
                        # è®¡ç®—æ–‡ä»¶å†…å®¹å“ˆå¸Œ
                        content_hash = self._calculate_file_hash(file_info.path)

                        # ä½¿ç”¨ç»Ÿä¸€é…ç½®è·å–æ–‡ä»¶ç±»å‹
                        try:
                            file_type = settings.default.get_file_type(file_info.extension)
                        except ValueError:
                            # å¦‚æœæ‰©å±•åä¸æ”¯æŒï¼Œå°è¯•ä»mime_typeæ¨æ–­
                            file_type = 'unknown'
                            if file_info.mime_type:
                                if 'image' in file_info.mime_type:
                                    file_type = 'image'
                                elif 'video' in file_info.mime_type:
                                    file_type = 'video'
                                elif 'audio' in file_info.mime_type:
                                    file_type = 'audio'
                                elif 'application/pdf' in file_info.mime_type or 'application/' in file_info.mime_type or 'text' in file_info.mime_type:
                                    file_type = 'document'

                        # è·³è¿‡ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹
                        if file_type == 'unknown':
                            logger.warning(f"è·³è¿‡ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_info.path} (æ‰©å±•å: {file_info.extension}, MIMEç±»å‹: {file_info.mime_type})")
                            continue

                        # åˆ›å»ºæˆ–æ›´æ–°æ–‡ä»¶è®°å½•
                        file_record = FileModel(
                            file_path=file_info.path,
                            file_name=file_info.name,
                            file_extension=file_info.extension,
                            file_type=file_type,
                            file_size=file_info.size,
                            created_at=file_info.created_time,
                            modified_at=file_info.modified_time,
                            indexed_at=datetime.now(),
                            content_hash=content_hash,
                            is_indexed=True,
                            is_content_parsed=True,
                            index_status='completed',
                            mime_type=file_info.mime_type,
                            title=document.get('title', ''),
                            author=document.get('author', ''),
                            keywords=document.get('keywords', ''),
                            content_length=len(document.get('content', '')),
                            word_count=len(document.get('content', '').split()),
                            parse_confidence=1.0,  # ç®€åŒ–å¤„ç†
                            index_quality_score=1.0,
                            needs_reindex=False,
                            # v2.0åˆ†å—å­—æ®µï¼ˆåˆå§‹å€¼ï¼Œå°†åœ¨åˆ†å—å¤„ç†åæ›´æ–°ï¼‰
                            is_chunked=self._should_be_chunked(len(document.get('content', ''))),
                            total_chunks=1,
                            chunk_strategy='500+50',
                            avg_chunk_size=500
                        )

                        # åˆå¹¶å¤„ç†ï¼šå¦‚æœæ–‡ä»¶å·²å­˜åœ¨åˆ™æ›´æ–°ï¼Œå¦åˆ™åˆ›å»º
                        existing_file = db.query(FileModel).filter(
                            FileModel.file_path == file_info.path
                        ).first()

                        if existing_file:
                            # æ›´æ–°ç°æœ‰è®°å½•
                            for key, value in file_record.__dict__.items():
                                if key != 'id' and not key.startswith('_'):
                                    setattr(existing_file, key, value)
                            db_file = existing_file
                        else:
                            # åˆ›å»ºæ–°è®°å½•
                            db.add(file_record)
                            db.flush()  # è·å–ID
                            db_file = file_record

                        # æ›´æ–°æ–‡æ¡£ä¸­çš„idä¸ºæ•°æ®åº“æ•´æ•°IDï¼Œä¾›åˆ†å—æœåŠ¡ä½¿ç”¨
                        document['id'] = db_file.id

                        # åˆ›å»ºæ–‡ä»¶å†…å®¹è®°å½•ï¼ˆå³ä½¿å†…å®¹ä¸ºç©ºä¹Ÿåˆ›å»ºï¼Œç”¨äºè·Ÿè¸ªå¤„ç†çŠ¶æ€ï¼‰
                        content_text = document.get('content', '')
                        has_error = 'error' in document.get('metadata', {})
                        error_message = document.get('metadata', {}).get('error', '') if has_error else ''

                        content_record = FileContentModel(
                            file_id=db_file.id,
                            title=document.get('title', ''),
                            content=content_text,
                            content_length=len(content_text),
                            word_count=len(content_text.split()) if content_text.strip() else 0,
                            language=document.get('language', 'unknown'),
                            confidence=document.get('confidence', 1.0),
                            is_parsed=not has_error,
                            has_error=has_error,
                            error_message=error_message,
                            parsed_at=datetime.now(),
                            updated_at=datetime.now()
                        )

                        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨å†…å®¹è®°å½•
                        existing_content = db.query(FileContentModel).filter(
                            FileContentModel.file_id == db_file.id
                        ).first()

                        if existing_content:
                            # æ›´æ–°ç°æœ‰è®°å½•
                            for key, value in content_record.__dict__.items():
                                if key != 'id' and not key.startswith('_'):
                                    setattr(existing_content, key, value)
                        else:
                            db.add(content_record)

                        # å®šæœŸæäº¤ä»¥é¿å…å†…å­˜å ç”¨è¿‡å¤§
                        if (i + 1) % 10 == 0:
                            db.commit()
                            logger.debug(f"å·²ä¿å­˜ {i + 1}/{len(documents)} ä¸ªæ–‡ä»¶")

                    except Exception as e:
                        logger.error(f"ä¿å­˜æ–‡ä»¶åˆ°æ•°æ®åº“å¤±è´¥ {file_info.path}: {e}")
                        continue

                # æœ€ç»ˆæäº¤
                db.commit()
                logger.info(f"æˆåŠŸä¿å­˜ {len(documents)} ä¸ªæ–‡ä»¶åˆ°æ•°æ®åº“")

            finally:
                db.close()

        except Exception as e:
            logger.error(f"ä¿å­˜æ–‡ä»¶æ•°æ®åˆ°æ•°æ®åº“å¤±è´¥: {e}")

    def _calculate_file_hash(self, file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶å†…å®¹çš„SHA256å“ˆå¸Œå€¼"""
        try:
            import hashlib
            hash_sha256 = hashlib.sha256()

            # å¯¹äºå¤§æ–‡ä»¶ï¼Œè¯»å–å‰1MBæ¥è®¡ç®—å“ˆå¸Œï¼ˆæé«˜æ€§èƒ½ï¼‰
            with open(file_path, "rb") as f:
                # è¯»å–å‰1MB
                chunk = f.read(1024 * 1024)
                if chunk:
                    hash_sha256.update(chunk)
                else:
                    # ç©ºæ–‡ä»¶
                    hash_sha256.update(b'')

            return hash_sha256.hexdigest()
        except Exception as e:
            logger.warning(f"è®¡ç®—æ–‡ä»¶å“ˆå¸Œå¤±è´¥ {file_path}: {e}")
            return ""

    async def _process_file_to_document(self, file_info: FileInfo) -> Optional[Dict[str, Any]]:
        """å°†æ–‡ä»¶ä¿¡æ¯å¤„ç†ä¸ºç´¢å¼•æ–‡æ¡£

        Args:
            file_info: æ–‡ä»¶ä¿¡æ¯

        Returns:
            Optional[Dict[str, Any]]: æ–‡æ¡£æ•°æ®
        """
        try:
            # 1. æå–å…ƒæ•°æ®
            metadata = self.metadata_extractor.extract_metadata(file_info.path)
            if 'error' in metadata:
                logger.warning(f"æå–å…ƒæ•°æ®å¤±è´¥ {file_info.path}: {metadata['error']}")

            # 2. è§£æå†…å®¹ï¼ˆæ”¯æŒå¼‚æ­¥ï¼‰
            parsed_content = await self.content_parser.parse_content(file_info.path)
            if hasattr(parsed_content, 'error') and parsed_content.error:
                logger.warning(f"è§£æå†…å®¹å¤±è´¥ {file_info.path}: {parsed_content.error}")

            # 3. æ„å»ºæ–‡æ¡£
            doc_id = self._generate_document_id(file_info)

            document = {
                'id': doc_id,
                'title': parsed_content.title or file_info.name,
                'content': parsed_content.text,
                'file_path': file_info.path,
                'file_name': file_info.name,
                'file_extension': file_info.extension,
                'file_type': metadata.get('file_type', 'unknown'),
                'file_size': file_info.size,
                'mime_type': file_info.mime_type,
                'created_time': file_info.created_time,
                'modified_time': file_info.modified_time,
                'language': parsed_content.language,
                'encoding': parsed_content.encoding,
                'content_hash': file_info.content_hash,
                'tags': self._extract_tags(metadata),
                'metadata': {
                    'extraction_confidence': parsed_content.confidence,
                    'content_length': len(parsed_content.text),
                    **metadata
                }
            }

            return document

        except Exception as e:
            logger.error(f"å¤„ç†æ–‡ä»¶åˆ°æ–‡æ¡£å¤±è´¥ {file_info.path}: {e}")
            return None

    def _generate_document_id(self, file_info: FileInfo) -> str:
        """ç”Ÿæˆæ–‡æ¡£ID"""
        # ä½¿ç”¨æ–‡ä»¶è·¯å¾„å’Œä¿®æ”¹æ—¶é—´ç”Ÿæˆå”¯ä¸€ID
        base_id = f"{file_info.path}_{file_info.modified_time.timestamp()}"
        import hashlib
        return hashlib.md5(base_id.encode('utf-8')).hexdigest()

    def _extract_tags(self, metadata: Dict[str, Any]) -> List[str]:
        """ä»å…ƒæ•°æ®ä¸­æå–æ ‡ç­¾"""
        tags = []

        # ä»æ–‡ä»¶ç±»å‹æå–æ ‡ç­¾
        file_type = metadata.get('file_type', '')
        if file_type:
            tags.append(file_type)

        # ä»MIMEç±»å‹æå–æ ‡ç­¾
        mime_type = metadata.get('mime_type', '')
        if mime_type:
            main_type = mime_type.split('/')[0]
            if main_type and main_type not in tags:
                tags.append(main_type)

        # ä»æ–‡æ¡£å±æ€§æå–æ ‡ç­¾
        if metadata.get('keywords'):
            keywords = metadata['keywords'].split(',')
            tags.extend([kw.strip() for kw in keywords if kw.strip()])

        # ä»å…¶ä»–å­—æ®µæå–æ ‡ç­¾
        for field in ['category', 'author']:
            if metadata.get(field):
                tags.append(str(metadata[field]))

        return list(set(tags))  # å»é‡

    def _update_indexed_files_cache(self, files: List[FileInfo]):
        """æ›´æ–°å·²ç´¢å¼•æ–‡ä»¶ç¼“å­˜"""
        self._indexed_files_cache.clear()
        for file_info in files:
            self._indexed_files_cache[file_info.path] = file_info

    def _update_progress(self, current: int, total: int, callback: Optional[callable], stage: str = ""):
        """æ›´æ–°è¿›åº¦"""
        if total > 0:
            progress = (current / total) * 30.0  # æ‰«æé˜¶æ®µå 30%
            self.index_status['indexing_progress'] = progress
            if callback:
                callback(f"{stage}: {current}/{total}", progress)

    def get_index_status(self) -> Dict[str, Any]:
        """è·å–ç´¢å¼•çŠ¶æ€"""
        status = self.index_status.copy()

        # æ·»åŠ åˆ†å—ç´¢å¼•ç»Ÿè®¡
        try:
            chunk_index_service = get_chunk_index_service()
            chunk_stats = chunk_index_service.get_index_stats()
            status.update({
                'chunk_faiss_index_exists': chunk_stats.get('chunk_faiss_index_exists', False),
                'chunk_whoosh_index_exists': chunk_stats.get('chunk_whoosh_index_exists', []),
                'total_chunks_created': chunk_stats.get('total_chunks_created', 0),
                'chunk_faiss_index_size': chunk_stats.get('chunk_faiss_index_size', 0)
            })
        except Exception as e:
            logger.warning(f"è·å–åˆ†å—ç´¢å¼•ç»Ÿè®¡å¤±è´¥: {e}")

        # æ·»åŠ ç¼“å­˜çŠ¶æ€ä¿¡æ¯
        status.update({
            'cached_files_count': len(self._indexed_files_cache)
        })

        return status

    def search_files(
        self,
        query: str,
        limit: int = 20,
        offset: int = 0,
        filters: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """æœç´¢æ–‡ä»¶

        æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„æœç´¢å®ç°ï¼Œå®é™…åº”è¯¥ä½¿ç”¨ä¸“é—¨çš„æœç´¢æœåŠ¡

        Args:
            query: æœç´¢æŸ¥è¯¢
            limit: è¿”å›ç»“æœæ•°é‡é™åˆ¶
            offset: ç»“æœåç§»é‡
            filters: è¿‡æ»¤æ¡ä»¶

        Returns:
            Dict[str, Any]: æœç´¢ç»“æœ
        """
        try:
            # è¿™é‡Œåº”è¯¥è°ƒç”¨ä¸“é—¨çš„æœç´¢æœåŠ¡
            # ç›®å‰è¿”å›ä¸€ä¸ªæ¨¡æ‹Ÿç»“æœ
            return {
                'success': True,
                'query': query,
                'total_found': 0,
                'results': [],
                'limit': limit,
                'offset': offset,
                'message': 'æœç´¢åŠŸèƒ½éœ€è¦ä¸“é—¨çš„æœç´¢æœåŠ¡å®ç°'
            }
        except Exception as e:
            logger.error(f"æœç´¢æ–‡ä»¶å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e)
            }

    async def delete_file_from_index(self, file_path: str) -> Dict[str, Any]:
        """ä»ç´¢å¼•ä¸­åˆ é™¤æ–‡ä»¶

        Args:
            file_path: æ–‡ä»¶è·¯å¾„

        Returns:
            Dict[str, Any]: åˆ é™¤ç»“æœ
        """
        try:
            # ç”Ÿæˆæ–‡æ¡£ID
            doc_id = self._generate_document_id_from_path(file_path)

            chunk_index_service = get_chunk_index_service()
            delete_result = await chunk_index_service.delete_file_from_indexes(file_path)
            success = delete_result.get('success', False)

            # ä»ç¼“å­˜ä¸­åˆ é™¤
            self._indexed_files_cache.pop(file_path, None)

            if not success:
                logger.error(f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {delete_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                return {
                    'success': False,
                    'error': delete_result.get('error', 'åˆ é™¤æ–‡ä»¶å¤±è´¥'),
                    'file_path': file_path
                }

            logger.info(f"åˆ†å—ç´¢å¼•æœåŠ¡åˆ é™¤æ–‡ä»¶æˆåŠŸ: {delete_result.get('deleted_chunks', 0)} ä¸ªåˆ†å—")

            return {
                'success': True,
                'file_path': file_path,
                'deleted_chunks': delete_result.get('deleted_chunks', 0),
                'faiss_deleted_count': delete_result.get('faiss_deleted_count', 0),
                'whoosh_deleted_count': delete_result.get('whoosh_deleted_count', 0),
                'duration_seconds': delete_result.get('duration_seconds', 0),
                'message': 'æ–‡ä»¶åˆ é™¤å®Œæˆ'
            }
        except Exception as e:
            logger.error(f"ä»ç´¢å¼•ä¸­åˆ é™¤æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return {
                'success': False,
                'error': str(e)
            }

    def _generate_document_id_from_path(self, file_path: str) -> str:
        """ä»æ–‡ä»¶è·¯å¾„ç”Ÿæˆæ–‡æ¡£ID"""
        try:
            path = Path(file_path)
            if path.exists():
                stat = path.stat()
                base_id = f"{file_path}_{stat.st_mtime}"
            else:
                base_id = file_path
            import hashlib
            return hashlib.md5(base_id.encode('utf-8')).hexdigest()
        except Exception:
            import hashlib
            return hashlib.md5(file_path.encode('utf-8')).hexdigest()

    def backup_indexes(self, backup_name: Optional[str] = None) -> Dict[str, Any]:
        """å¤‡ä»½ç´¢å¼•

        Args:
            backup_name: å¤‡ä»½åç§°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨æ—¶é—´æˆ³

        Returns:
            Dict[str, Any]: å¤‡ä»½ç»“æœ
        """
        try:
            if not backup_name:
                backup_name = f"index_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

            backup_dir = self.data_root / "backups" / backup_name
            # æ³¨æ„ï¼šåˆ†å—ç´¢å¼•æœåŠ¡æš‚ä¸æä¾›å¤‡ä»½åŠŸèƒ½ï¼Œè¿™é‡Œä»…è®°å½•æ—¥å¿—
            logger.info(f"éœ€è¦å¤‡ä»½ç´¢å¼•åˆ°: {backup_dir}")
            success = True

            return {
                'success': success,
                'backup_path': str(backup_dir),
                'backup_name': backup_name
            }
        except Exception as e:
            logger.error(f"å¤‡ä»½ç´¢å¼•å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e)
            }

    def get_supported_formats(self) -> Dict[str, List[str]]:
        """è·å–æ”¯æŒçš„æ–‡ä»¶æ ¼å¼"""
        return {
            'scanner_formats': self.scanner.DEFAULT_SUPPORTED_EXTENSIONS,
            'parser_formats': self.content_parser.get_supported_formats(),
            'extractor_formats': self.metadata_extractor.get_supported_formats()
        }

    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            # æ¸…ç†ç¼“å­˜
            self._indexed_files_cache.clear()

            # è¿™é‡Œå¯ä»¥æ·»åŠ å…¶ä»–æ¸…ç†é€»è¾‘
            logger.info("æ–‡ä»¶ç´¢å¼•æœåŠ¡èµ„æºæ¸…ç†å®Œæˆ")
        except Exception as e:
            logger.error(f"æ¸…ç†èµ„æºå¤±è´¥: {e}")


# å…¨å±€æ–‡ä»¶ç´¢å¼•æœåŠ¡å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
_file_index_service: Optional[FileIndexService] = None


def get_file_index_service() -> FileIndexService:
    """è·å–æ–‡ä»¶ç´¢å¼•æœåŠ¡å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰

    Returns:
        FileIndexService: æ–‡ä»¶ç´¢å¼•æœåŠ¡å®ä¾‹
    """
    global _file_index_service
    if _file_index_service is None:
        from app.core.config import get_settings
        settings = get_settings()

        faiss_path, whoosh_path = settings.get_index_paths()
        _file_index_service = FileIndexService(
            data_root=settings.index.data_root,
            faiss_index_path=faiss_path,
            whoosh_index_path=whoosh_path,
            use_chinese_analyzer=settings.index.use_chinese_analyzer,
            scanner_config={
                'max_workers': settings.index.scanner_max_workers,
                'max_file_size': settings.index.max_file_size,
                'supported_extensions': set(settings.index.supported_extensions)
            },
            parser_config={
                'max_content_length': settings.index.max_content_length
            }
        )

        logger.info("æ–‡ä»¶ç´¢å¼•æœåŠ¡å®ä¾‹å·²åˆ›å»º")
    return _file_index_service
