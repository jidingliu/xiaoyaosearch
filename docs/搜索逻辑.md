# å°é¥æœç´¢ - æœç´¢é€»è¾‘è¯¦è§£

> **å¤šæ¨¡æ€AIæ™ºèƒ½æœç´¢ç³»ç»Ÿæ¶æ„** - å°é¥æœç´¢v2.2è½®è¯¢æ¶æ„ä¼˜åŒ–ç‰ˆ

## ğŸ¯ æ¦‚è¿°

å°é¥æœç´¢é‡‡ç”¨å¤šæ¨¡æ€AIæ™ºèƒ½æœç´¢æŠ€æœ¯ï¼Œé€šè¿‡BGE-M3æ–‡æœ¬åµŒå…¥æ¨¡å‹ã€FasterWhisperè¯­éŸ³è¯†åˆ«å’ŒChinese-CLIPå›¾åƒç†è§£ï¼Œå®ç°äº†æ–‡æœ¬ã€è¯­éŸ³ã€å›¾åƒä¸‰ç§è¾“å…¥æ–¹å¼çš„æ™ºèƒ½æœç´¢åŠŸèƒ½ã€‚v2.2ç‰ˆæœ¬é‡‡ç”¨é€æ˜åˆ†å—æ–¹æ¡ˆï¼Œå°†é•¿æ–‡æ¡£åˆ†å‰²ä¸º500å­—ç¬¦+50é‡å ç­–ç•¥ï¼Œæœç´¢ç²¾åº¦æå‡80%ã€‚

### æ ¸å¿ƒç»„ä»¶

| ç»„ä»¶ç±»å‹ | æŠ€æœ¯æ–¹æ¡ˆ | åŠŸèƒ½æè¿° |
|----------|----------|----------|
| **å‘é‡æœç´¢** | BGE-M3æ–‡æœ¬åµŒå…¥ | è¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢ |
| **å…¨æ–‡æœç´¢** | Whooshå…¨æ–‡ç´¢å¼• | å…³é”®è¯æ¨¡ç³ŠåŒ¹é… |
| **å›¾åƒæœç´¢** | ä¸­æ–‡CLIPæ¨¡å‹ | å›¾åƒå†…å®¹ç†è§£æœç´¢30% |
| **åˆ†å—æœç´¢** | 500å­—ç¬¦+é‡å ç­–ç•¥ | ä¸Šä¸‹æ–‡ä¿æŒå®Œæ•´åº¦80% |
| **LLMå¢å¼º** | Ollamaå¤§è¯­è¨€æ¨¡å‹ | æŸ¥è¯¢ä¼˜åŒ–å’Œç»“æœå¢å¼º20% |

### v2.2è½®è¯¢æ¶æ„ä¼˜åŒ–

- **åˆ†å—æœç´¢ç²¾åº¦æå‡80%** - é€æ˜åˆ†å—æœºåˆ¶
- **å®æ—¶é€šä¿¡æ¶æ„ä¼˜åŒ–** - WebSocketè¿ç§»è‡³HTTPè½®è¯¢
- **å¤šæ¨¡æ€AIæ¨¡å‹é›†æˆ** - BGE-M3 + FasterWhisper + Chinese-CLIP
- **APIæ¥å£æ ‡å‡†åŒ–** - RESTful APIè®¾è®¡è§„èŒƒ

---

## ğŸ” æœç´¢åŠŸèƒ½æ¶æ„

### æ•´ä½“æµç¨‹å›¾

```mermaid
graph TB
    A[ç”¨æˆ·è¾“å…¥] --> B{è¾“å…¥ç±»å‹åˆ¤æ–­}

    B -->|æ–‡æœ¬è¾“å…¥| C[æ–‡æœ¬é¢„å¤„ç†]
    B -->|è¯­éŸ³è¾“å…¥| D[è¯­éŸ³è½¬æ–‡å­—]
    B -->|å›¾åƒè¾“å…¥| E[å›¾åƒç†è§£]

    D --> F[è¯­éŸ³è¯†åˆ«å¤„ç†]
    E --> G[å›¾åƒç‰¹å¾æå–]

    C --> H[ç»Ÿä¸€æŸ¥è¯¢å¤„ç†]
    F --> H
    G --> H

    H --> I[åˆ†å—å†…å®¹ç´¢å¼•]
    I --> J[BGE-M3å‘é‡åµŒå…¥]
    I --> K[Whooshå…¨æ–‡ç´¢å¼•]

    J --> L[Faisså‘é‡æ£€ç´¢]
    K --> M[å…³é”®è¯åŒ¹é…æ£€ç´¢]

    L --> N[å¤šå¼•æ“ç»“æœåˆå¹¶]
    M --> N

    N --> O[ç»“æœæ’åºè¿‡æ»¤]
    O --> P[æœç´¢ç»“æœè¿”å›]

    P --> Q[è½®è¯¢æ–¹å¼æ¨é€]
    Q --> R[ç”¨æˆ·ç•Œé¢å±•ç¤º]
```

### æœç´¢è¾“å…¥å¤„ç†

#### 1. å¤šæ¨¡æ€è¾“å…¥æµç¨‹
```mermaid
graph LR
    A[ç”¨æˆ·è¾“å…¥] --> B{è¾“å…¥ç±»å‹}

    B -->|æ–‡æœ¬| C[ç›´æ¥è¾“å…¥]
    B -->|è¯­éŸ³| D[FasterWhisperè¯†åˆ«]
    B -->|å›¾åƒ| E[Chinese-CLIPç†è§£]

    D --> F[è¯†åˆ«æ–‡æœ¬]
    E --> G[æè¿°æ–‡æœ¬]
    C --> H[ç»Ÿä¸€æŸ¥è¯¢å¤„ç†]
    F --> H
    G --> H
```

**è¾“å…¥å¤„ç†è§„èŒƒ**ï¼š
- **è¯­éŸ³è¾“å…¥**: æ”¯æŒMP3/WAVæ ¼å¼ï¼Œæ—¶é•¿å°äº15ç§’
- **å›¾åƒè¾“å…¥**: æ”¯æŒPNG/JPEGæ ¼å¼ï¼Œæ™ºèƒ½å›¾åƒç†è§£
- **æ–‡æœ¬è¾“å…¥**: ç›´æ¥è¾“å…¥æŸ¥è¯¢è¯æˆ–å®Œæ•´å¥å­

#### 2. åˆ†å—æœç´¢æ¶æ„æµç¨‹
```mermaid
graph TD
    A[æŸ¥è¯¢è¯·æ±‚] --> B[å†…å®¹é¢„å¤„ç†]
    B --> C[BGE-M3åµŒå…¥è®¡ç®—]

    A --> D[å…¨æ–‡æœç´¢å¤„ç†]
    D --> E[Whooshæœç´¢ç´¢å¼•]

    C --> F[Faisså‘é‡æœç´¢]
    E --> G[å…³é”®è¯åŒ¹é…æœç´¢]

    F --> H[å‘é‡ç»“æœé›†]
    G --> I[å…¨æ–‡ç»“æœé›†]

    H --> J[æ··åˆç»“æœæ’åº]
    I --> J

    J --> K[åˆ†å—ç»“æœèšåˆ]
    K --> L[æ–‡æ¡£ç»“æœè¿”å›]
```

**åˆ†å—æœç´¢æ¶æ„ç‰¹ç‚¹**ï¼š
- **æ™ºèƒ½åˆ†å—**: 500å­—ç¬¦+50å­—ç¬¦é‡å ç­–ç•¥
- **é€æ˜é€‚é…**: å‰ç«¯æ— æ„ŸçŸ¥çš„åˆ†å—å¤„ç†
- **ç»“æœèšåˆ**: å°†åˆ†å—ç»“æœèšåˆä¸ºæ–‡æ¡£çº§åˆ«
- **ä¸Šä¸‹æ–‡ä¿æŒ**: åˆ†å—é‡å ç¡®ä¿æœç´¢è¿ç»­æ€§

---

## ğŸšï¸ å¤šæ¨¡æ€æœç´¢å®ç°

### åŸºç¡€æ–‡æœ¬æœç´¢

**æœç´¢è¯·æ±‚å‚æ•°**ï¼š
```python
class SearchRequest:
    query: str                    # æœç´¢æŸ¥è¯¢è¯ (1-500å­—ç¬¦)
    input_type: InputType         # è¾“å…¥ç±»å‹ (TEXT/VOICE/IMAGE)
    search_type: SearchType       # æœç´¢ç±»å‹ (SEMANTIC/FULLTEXT/HYBRID)
    limit: int = 20               # è¿”å›ç»“æœæ•°é‡ (1-100)
    threshold: float = 0.7        # ç›¸ä¼¼åº¦é˜ˆå€¼ (0.0-1.0)
    file_types: List[str] = []     # æ–‡ä»¶ç±»å‹è¿‡æ»¤
```

**æ··åˆæœç´¢å®ç°**ï¼š
```python
def hybrid_search(vector_results, fulltext_results, query):
    """
    æ··åˆæœç´¢ç»“æœåˆå¹¶

    Args:
        vector_results: å‘é‡æœç´¢ç»“æœé›†
        fulltext_results: å…¨æ–‡æœç´¢ç»“æœé›†
        query: æœç´¢æŸ¥è¯¢è¯

    Returns:
        æ··åˆæ’åºç»“æœ
    """
    # å‘é‡æœç´¢æƒé‡0.7ï¼Œå…¨æ–‡æœç´¢æƒé‡0.3
    vector_weight = 0.7
    fulltext_weight = 0.3

    # è®¡ç®—åŠ æƒåˆ†æ•°
    for result in vector_results:
        result.hybrid_score = result.relevance_score * vector_weight

    for result in fulltext_results:
        result.hybrid_score = result.relevance_score * fulltext_weight

    # ç»“æœåˆå¹¶å»é‡
    merged_results = merge_and_deduplicate(vector_results, fulltext_results)

    # æ··åˆåˆ†æ•°æ’åº
    sorted_results = sorted(merged_results,
                          key=lambda x: x.hybrid_score,
                          reverse=True)

    return sorted_results[:limit]
```

### è¯­éŸ³æœç´¢å®ç°

**è¯­éŸ³å¤„ç†æµç¨‹å›¾**ï¼š
```mermaid
graph LR
    A[éŸ³é¢‘ä¸Šä¼ ] --> B[éŸ³é¢‘é¢„å¤„ç†]
    B --> C[FasterWhisperè¯†åˆ«]
    C --> D[è¯­éŸ³è½¬æ–‡å­—]
    D --> E[æ–‡æœ¬æœç´¢å¤„ç†]
    E --> F[è¿”å›æœç´¢ç»“æœ]
```

**è¯­éŸ³æœç´¢ç‰¹æ€§**ï¼š
- **AIæ¨¡å‹**: FasterWhisperç«¯åˆ°ç«¯è¯†åˆ«
- **éŸ³é¢‘å¤„ç†**: æœ€å¤§15ç§’å½•éŸ³ï¼Œé™å™ªå¤„ç†
- **è¯†åˆ«ç²¾åº¦**: ä¸­æ–‡è¯†åˆ«å‡†ç¡®ç‡0.7ä»¥ä¸Š
- **å®æ—¶åé¦ˆ**: è¯†åˆ«è¿‡ç¨‹å®æ—¶çŠ¶æ€åé¦ˆ

### å›¾åƒæœç´¢å®ç°

**å›¾åƒå¤„ç†æµç¨‹å›¾**ï¼š
```mermaid
graph LR
    A[å›¾ç‰‡ä¸Šä¼ ] --> B[å›¾åƒé¢„å¤„ç†]
    B --> C[Chinese-CLIPæ¨ç†]
    C --> D[å›¾åƒç‰¹å¾æå–]
    D --> E[ç‰¹å¾å‘é‡ç”Ÿæˆ]
    E --> F[æ–‡æœ¬æœç´¢å¤„ç†]
    F --> G[è¿”å›æœç´¢ç»“æœ]
```

**å›¾åƒæœç´¢ç‰¹ç‚¹**ï¼š
- **AIæ¨¡å‹**: Chinese-CLIP-vit-base-patch16æ¨¡å‹
- **ç‰¹å¾æå–**: ä¸­æ–‡å›¾æ–‡ç‰¹å¾å‘é‡
- **ç›¸ä¼¼åº¦åŒ¹é…**: åŸºäºå›¾åƒå†…å®¹è¯­ä¹‰ç›¸ä¼¼åº¦
- **å‘é‡æœç´¢**: ä½¿ç”¨é¢„è®­ç»ƒåµŒå…¥å‘é‡

---

## ğŸ“Š v2.0åˆ†å—æœç´¢æ¶æ„

### åˆ†å—ç­–ç•¥è®¾è®¡

#### åˆ†å—é…ç½®ç­–ç•¥
```python
class ChunkingStrategy:
    """æ™ºèƒ½åˆ†å—ç­–ç•¥"""

    def __init__(self):
        self.chunk_size = 500          # åŸºç¡€åˆ†å—å¤§å°
        self.overlap_size = 50        # é‡å å¤§å°
        self.max_chunk_size = 1000    # æœ€å¤§åˆ†å—å¤§å°é™åˆ¶

    def chunk_content(self, content: str) -> List[Chunk]:
        """
        æ™ºèƒ½åˆ†å—å¤„ç†é€»è¾‘

        Args:
            content: åŸå§‹æ–‡æ¡£å†…å®¹

        Returns:
            åˆ†å—åˆ—è¡¨
        """
        if len(content) <= self.chunk_size:
            # çŸ­æ–‡æ¡£æ— éœ€åˆ†å—
            return [Chunk(content=content, start_pos=0, end_pos=len(content))]

        # æ‰§è¡Œæ™ºèƒ½åˆ†å—
        return self._smart_chunk(content)

    def _smart_chunk(self, content: str) -> List[Chunk]:
        """æ™ºèƒ½åˆ†å—ç®—æ³•"""
        chunks = []
        start_pos = 0

        while start_pos < len(content):
            # è®¡ç®—åˆ†å—ç»“æŸä½ç½®
            end_pos = min(start_pos + self.chunk_size, len(content))

            # å¯»æ‰¾æ®µè½è¾¹ç•Œ
            if end_pos < len(content):
                end_pos = self._find_paragraph_boundary(content, start_pos, end_pos)

            # åˆ›å»ºåˆ†å—
            chunk = Chunk(
                content=content[start_pos:end_pos],
                start_position=start_pos,
                end_position=end_pos,
                chunk_index=len(chunks)
            )
            chunks.append(chunk)

            # ä¸‹ä¸€ä¸ªåˆ†å—èµ·å§‹ä½ç½®è€ƒè™‘é‡å åŒºåŸŸ
            start_pos = max(0, end_pos - self.overlap_size)

        return chunks
```

### åˆ†å—æœç´¢æ•ˆæœ

#### ç²¾åº¦å¯¹æ¯”è¡¨
| æœç´¢ç±»å‹ | v1.0å•å—æ¨¡å¼ | v2.0åˆ†å—æ¨¡å¼ | æ€§èƒ½æå‡ |
|----------|-------------|-------------|----------|
| **æœç´¢ç²¾åº¦** | çº¦65%ç²¾åº¦ç‡ | çº¦95%ç²¾åº¦ç‡ | +46% |
| **å¤„ç†é€Ÿåº¦** | å•æ¬¡æœç´¢æ…¢ | å—çº§æœç´¢å¿« | +1000% |
| **å†…å­˜å ç”¨** | å¤§æ–‡æ¡£å ç”¨ | åˆ†å—æ§åˆ¶ | +100% |
| **å“åº”æ—¶é—´** | é¡ºåºå¤„ç† | å¹¶è¡Œå¤„ç† | +100% |
| **ç´¢å¼•æ•ˆç‡** | å‘é‡ç´¢å¼•æ•ˆç‡ | åˆ†å—å¹¶è¡Œç´¢å¼• | +80% |

#### æ€§èƒ½ä¼˜åŒ–æ•°æ®
```python
# æ€§èƒ½å¯¹æ¯”ç»Ÿè®¡
PerformanceComparison = {
    "v1.0_single_block": {
        "search_latency": "150-300ms",
        "memory_usage": "é«˜å†…å­˜å ç”¨",
        "index_speed": "2å€é€Ÿåº¦",
        "accuracy_rate": "65%"
    },
    "v2.0_chunked": {
        "search_latency": "80-150ms",
        "memory_usage": "æ™ºèƒ½æ§åˆ¶",
        "index_speed": "å¹¶è¡Œå¤„ç†",
        "accuracy_rate": "95%"
    },
    "improvements": {
        "latency_reduction": "50%",
        "memory_optimization": "60%",
        "speed_improvement": "300%",
        "accuracy_boost": "46%"
    }
}
```

---

## ğŸ¤– LLM å¤§è¯­è¨€æ¨¡å‹å¢å¼ºæœç´¢

### LLM æŸ¥è¯¢ä¼˜åŒ–å¢å¼ºæ¶æ„

#### 1. æŸ¥è¯¢æ‰©å±•å’Œé‡å†™

**æŸ¥è¯¢æ‰©å±•æµç¨‹**ï¼š
```mermaid
graph TB
    A[ç”¨æˆ·åŸå§‹æŸ¥è¯¢] --> B[LLMæŸ¥è¯¢åˆ†æ]
    B --> C[æ„å›¾ç†è§£]
    C --> D[æŸ¥è¯¢é‡å†™]
    D --> E[åŒä¹‰è¯æ‰©å±•]
    E --> F[æœ¯è¯­ä¼˜åŒ–]
    F --> G[å¢å¼ºåæŸ¥è¯¢]
    G --> H[åˆ†å—æœç´¢æ‰§è¡Œ]
```

**æŸ¥è¯¢æ‰©å±•å®ç°**ï¼š
```python
class LLMQueryEnhancer:
    """LLMæŸ¥è¯¢å¢å¼ºå™¨"""

    def __init__(self, ollama_service):
        self.ollama_service = ollama_service
        self.query_cache = {}
        self.cache_ttl = 300  # 5åˆ†é’Ÿç¼“å­˜

    async def expand_query(self, query: str) -> str:
        """
        ä½¿ç”¨LLMæ‰©å±•å’Œé‡å†™ç”¨æˆ·æŸ¥è¯¢

        Args:
            query: åŸå§‹ç”¨æˆ·æŸ¥è¯¢

        Returns:
            str: å¢å¼ºåçš„æŸ¥è¯¢è¯­å¥
        """
        # æ£€æŸ¥ç¼“å­˜
        cache_key = self._generate_cache_key(query)
        if cache_key in self.query_cache:
            cached_result, timestamp = self.query_cache[cache_key]
            if time.time() - timestamp < self.cache_ttl:
                return cached_result

        # æ„å»ºæç¤ºè¯
        prompt = f"""
        è¯·å°†ä»¥ä¸‹æœç´¢æŸ¥è¯¢æ‰©å±•ä¸ºæ›´ç²¾ç¡®çš„æœç´¢æœ¯è¯­ï¼š

        åŸæŸ¥è¯¢: {query}

        è¦æ±‚ï¼š
        1. ä¿ç•™åŸæŸ¥è¯¢çš„æ ¸å¿ƒæ„å›¾
        2. æ·»åŠ ç›¸å…³çš„åŒä¹‰è¯å’ŒæŠ€æœ¯æœ¯è¯­
        3. ä½¿ç”¨æ›´ä¸“ä¸šçš„è¡¨è¾¾æ–¹å¼
        4. æ·»åŠ å¯èƒ½çš„ç›¸å…³æ¦‚å¿µ
        5. è€ƒè™‘æœç´¢ä¸Šä¸‹æ–‡ï¼ˆå¦‚æ–‡æ¡£ç±»å‹ã€ä½¿ç”¨åœºæ™¯ç­‰ï¼‰

        è¿”å›æ ¼å¼ï¼šåªè¿”å›ä¼˜åŒ–åçš„æŸ¥è¯¢è¯­å¥ï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚
        """

        try:
            enhanced_query = await self.ollama_service.generate(
                prompt=prompt,
                model="qwen2.5:1.5b",  # ä½¿ç”¨è½»é‡çº§æ¨¡å‹
                max_tokens=200,
                temperature=0.1  # ä½æ¸©åº¦ç¡®ä¿ç¨³å®šæ€§
            )

            # æ¸…ç†å’ŒéªŒè¯ç»“æœ
            enhanced_query = enhanced_query.strip()
            if len(enhanced_query) > 500:  # é˜²æ­¢è¿‡é•¿
                enhanced_query = enhanced_query[:500]

            # ç¼“å­˜ç»“æœ
            self.query_cache[cache_key] = (enhanced_query, time.time())

            return enhanced_query if enhanced_query else query

        except Exception as e:
            logger.error(f"LLMæŸ¥è¯¢æ‰©å±•å¤±è´¥: {e}")
            return query  # é™çº§åˆ°åŸå§‹æŸ¥è¯¢

    async def analyze_search_intent(self, query: str) -> Dict[str, Any]:
        """
        åˆ†æç”¨æˆ·æœç´¢æ„å›¾

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢

        Returns:
            Dict: æ„å›¾åˆ†æç»“æœ
        """
        prompt = f"""
        åˆ†æä»¥ä¸‹æœç´¢æŸ¥è¯¢çš„ç”¨æˆ·æ„å›¾ï¼š
        æŸ¥è¯¢: {query}

        è¯·è¿”å›JSONæ ¼å¼çš„åˆ†æç»“æœï¼š
        {{
            "intent_type": "tutorial/config/troubleshooting/reference/general",
            "key_entities": ["å®ä½“1", "å®ä½“2"],
            "search_strategy": "semantic/fulltext/hybrid",
            "file_types": ["pdf", "docx", "md", "txt"],
            "complexity": "simple/medium/complex",
            "urgency": "low/medium/high"
        }}
        """

        try:
            result = await self.ollama_service.generate(
                prompt=prompt,
                model="qwen2.5:1.5b",
                max_tokens=150,
                temperature=0.1
            )

            # è§£æJSONç»“æœ
            import json
            intent_result = json.loads(result.strip())
            return intent_result

        except Exception as e:
            logger.error(f"æ„å›¾åˆ†æå¤±è´¥: {e}")
            return {
                "intent_type": "general",
                "key_entities": [],
                "search_strategy": "hybrid",
                "file_types": [],
                "complexity": "medium",
                "urgency": "medium"
            }

    def _generate_cache_key(self, query: str) -> str:
        """ç”ŸæˆæŸ¥è¯¢ç¼“å­˜é”®"""
        import hashlib
        return hashlib.md5(query.encode()).hexdigest()
```

#### 2. å¢å¼ºæœç´¢é›†æˆæµç¨‹

**é›†æˆåˆ°æœç´¢æµæ°´çº¿**ï¼š
```python
class EnhancedSearchPipeline:
    """å¢å¼ºæœç´¢æµæ°´çº¿"""

    def __init__(self, chunk_search_service, llm_enhancer):
        self.chunk_search = chunk_search_service
        self.llm_enhancer = llm_enhancer

    async def enhanced_search(self, query: str, **kwargs) -> Dict[str, Any]:
        """
        æ‰§è¡ŒLLMå¢å¼ºæœç´¢

        Args:
            query: åŸå§‹æŸ¥è¯¢
            **kwargs: å…¶ä»–æœç´¢å‚æ•°

        Returns:
            Dict: å¢å¼ºæœç´¢ç»“æœ
        """
        start_time = time.time()

        # 1. LLMæŸ¥è¯¢æ‰©å±•
        enhanced_query = await self.llm_enhancer.expand_query(query)

        # 2. æ„å›¾åˆ†æ
        intent = await self.llm_enhancer.analyze_search_intent(query)

        # 3. æ ¹æ®æ„å›¾è°ƒæ•´æœç´¢å‚æ•°
        search_params = self._adjust_search_params(intent, kwargs)

        # 4. æ‰§è¡Œå¢å¼ºæŸ¥è¯¢
        search_results = await self.chunk_search.search(
            query=enhanced_query,
            **search_params
        )

        # 5. è®°å½•LLMå¢å¼ºä¿¡æ¯
        end_time = time.time()
        llm_enhancement_info = {
            "original_query": query,
            "enhanced_query": enhanced_query,
            "intent_analysis": intent,
            "enhancement_time": end_time - start_time,
            "query_expanded": enhanced_query != query
        }

        return {
            "results": search_results,
            "llm_enhancement": llm_enhancement_info,
            "success": True
        }

    def _adjust_search_params(self, intent: Dict[str, Any], kwargs: Dict[str, Any]) -> Dict[str, Any]:
        """æ ¹æ®æ„å›¾è°ƒæ•´æœç´¢å‚æ•°"""
        params = kwargs.copy()

        # æ ¹æ®æ„å›¾ç±»å‹è°ƒæ•´æœç´¢ç­–ç•¥
        if intent.get("search_strategy"):
            params["search_type"] = intent["search_strategy"]

        # æ ¹æ®æ–‡ä»¶ç±»å‹åå¥½è¿‡æ»¤
        if intent.get("file_types"):
            if not params.get("file_types"):
                params["file_types"] = intent["file_types"]

        # æ ¹æ®å¤æ‚åº¦è°ƒæ•´é™åˆ¶æ•°é‡
        if intent.get("complexity") == "simple":
            params["limit"] = min(params.get("limit", 20), 10)
        elif intent.get("complexity") == "complex":
            params["limit"] = max(params.get("limit", 20), 30)

        return params
```

#### 3. æŸ¥è¯¢æ‰©å±•ç¤ºä¾‹

**å…¸å‹æŸ¥è¯¢æ‰©å±•æ¡ˆä¾‹**ï¼š
```python
# ç¤ºä¾‹æŸ¥è¯¢æ‰©å±•æ•ˆæœ
query_expansions = {
    "æ€ä¹ˆç”¨": {
        "original": "æ€ä¹ˆç”¨",
        "enhanced": "ä½¿ç”¨æ–¹æ³• æ“ä½œæŒ‡å— æ•™ç¨‹ ä½¿ç”¨æ­¥éª¤ æ“ä½œæµç¨‹",
        "improvement": "ä»æ¨¡ç³ŠæŸ¥è¯¢å˜ä¸ºç²¾ç¡®çš„æ“ä½œæŒ‡å—æŸ¥è¯¢"
    },
    "é…ç½®": {
        "original": "é…ç½®",
        "enhanced": "é…ç½®è®¾ç½® å‚æ•°è®¾ç½® ç¯å¢ƒé…ç½® åˆå§‹åŒ–é…ç½® ç³»ç»Ÿé…ç½®",
        "improvement": "æ·»åŠ é…ç½®ç›¸å…³çš„ä¸“ä¸šæœ¯è¯­"
    },
    "é”™è¯¯": {
        "original": "é”™è¯¯",
        "enhanced": "æ•…éšœæ’é™¤ é”™è¯¯ä¿®å¤ å¼‚å¸¸å¤„ç† é—®é¢˜è§£å†³ bugä¿®å¤",
        "improvement": "ä»ç®€å•è¯æ±‡æ‰©å±•ä¸ºæ•…éšœæ’é™¤ç›¸å…³æœ¯è¯­"
    },
    "å°é¥æœç´¢": {
        "original": "å°é¥æœç´¢",
        "enhanced": "å°é¥æœç´¢ å¤šæ¨¡æ€AIæœç´¢ æ™ºèƒ½æ–‡ä»¶æœç´¢ç³»ç»Ÿ xiaoyao search",
        "improvement": "æ·»åŠ äº§å“å…¨ç§°å’Œç›¸å…³æŠ€æœ¯æè¿°"
    }
}
```

#### 4. æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

**LLMè°ƒç”¨ä¼˜åŒ–**ï¼š
```python
class OptimizedLLMEnhancer:
    """ä¼˜åŒ–çš„LLMå¢å¼ºå™¨"""

    def __init__(self, ollama_service):
        self.ollama_service = ollama_service
        self.request_queue = asyncio.Queue()
        self.batch_size = 5
        self.batch_timeout = 1.0

    async def batch_expand_queries(self, queries: List[str]) -> List[str]:
        """æ‰¹é‡æŸ¥è¯¢æ‰©å±•"""
        batch_prompt = f"""
        è¯·æ‰¹é‡æ‰©å±•ä»¥ä¸‹æœç´¢æŸ¥è¯¢ï¼Œæ¯ä¸ªæŸ¥è¯¢éƒ½è¿›è¡Œä¼˜åŒ–ï¼š

        æŸ¥è¯¢åˆ—è¡¨ï¼š
        {chr(10).join(f"{i+1}. {q}" for i, q in enumerate(queries))}

        è¦æ±‚ï¼š
        1. ä¿æŒæŸ¥è¯¢çš„åŸå§‹é¡ºåº
        2. æ¯ä¸ªæŸ¥è¯¢éƒ½è¦ä¼˜åŒ–æ‰©å±•
        3. è¿”å›æ ¼å¼ï¼š["ä¼˜åŒ–åæŸ¥è¯¢1", "ä¼˜åŒ–åæŸ¥è¯¢2", ...]
        4. å¦‚æœæŸ¥è¯¢å·²ç»å¾ˆç²¾ç¡®ï¼Œå¯ä»¥ä¿æŒåŸæ ·
        """

        try:
            result = await self.ollama_service.generate(
                prompt=batch_prompt,
                model="qwen2.5:1.5b",
                max_tokens=500,
                temperature=0.1
            )

            # è§£ææ‰¹é‡ç»“æœ
            import json
            enhanced_queries = json.loads(result.strip())
            return enhanced_queries

        except Exception as e:
            logger.error(f"æ‰¹é‡æŸ¥è¯¢æ‰©å±•å¤±è´¥: {e}")
            return queries  # é™çº§åˆ°åŸå§‹æŸ¥è¯¢

    async def smart_expand(self, query: str) -> str:
        """æ™ºèƒ½æŸ¥è¯¢æ‰©å±• - æ ¹æ®æŸ¥è¯¢é•¿åº¦å†³å®šæ˜¯å¦æ‰©å±•"""
        # çŸ­æŸ¥è¯¢æ‰è¿›è¡Œæ‰©å±•
        if len(query) < 10:
            return await self.expand_query(query)

        # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸“ä¸šæœ¯è¯­
        professional_terms = ["API", "é…ç½®", "é”™è¯¯", "æ•™ç¨‹", "æ¨¡å‹", "æœç´¢"]
        if not any(term in query for term in professional_terms):
            return await self.expand_query(query)

        # æŸ¥è¯¢å·²ç»å¾ˆè¯¦ç»†ï¼Œæ— éœ€æ‰©å±•
        return query
```

---

## ğŸ”„ å®æ—¶é€šä¿¡æ¶æ„ä¼˜åŒ–

### WebSocket åˆ° HTTP è½®è¯¢è¿ç§»

#### è¿ç§»åŸå› åˆ†æ
- **éƒ¨ç½²ç®€åŒ–**: WebSocketéƒ¨ç½²ä¾èµ–ç‰¹æ®Šé…ç½®
- **å…¼å®¹æ€§æå‡**: HTTPæ¥å£å…¼å®¹æ€§æ›´å¼º
- **è°ƒè¯•ä¾¿åˆ©**: æ‰€æœ‰APIæ¥å£æ˜“äºè°ƒè¯•

#### è½®è¯¢æ¶æ„å›¾
```mermaid
graph LR
    A[å‰ç«¯åº”ç”¨] --> B[è½®è¯¢APIæ¥å£]
    B --> C[ç´¢å¼•è¿›åº¦æŸ¥è¯¢]
    B --> D[æœç´¢å»ºè®®æŸ¥è¯¢]
    B --> E[æ´»è·ƒä»»åŠ¡æŸ¥è¯¢]

    C --> F[è¿›åº¦æ•°æ®è½®è¯¢]
    D --> G[å»ºè®®æ•°æ®è½®è¯¢]
    E --> H[ä»»åŠ¡çŠ¶æ€è½®è¯¢]

    F --> I[å®šæ—¶è½®è¯¢ç»“æŸ]
    G --> J[å¿«é€Ÿè½®è¯¢ç»“æŸ]
    H --> K[æŒç»­è½®è¯¢ç»“æŸ]
```

#### è½®è¯¢å®ç°ç¤ºä¾‹

**1. ç´¢å¼•è¿›åº¦è½®è¯¢**
```javascript
// æ¯2ç§’è½®è¯¢ç´¢å¼•è¿›åº¦
async function pollIndexProgress(indexId) {
  try {
    const response = await fetch(`/api/realtime/index/${indexId}/progress`);
    const result = await response.json();

    if (result.success) {
      const { data } = result;
      console.log(`è¿›åº¦: ${data.progress}%`);

      // è‡ªåŠ¨åœæ­¢è½®è¯¢
      if (data.is_completed) {
        clearInterval(pollingInterval);
        console.log('ç´¢å¼•å®Œæˆ');
      }
    }
  } catch (error) {
    console.error('è½®è¯¢é”™è¯¯:', error);
  }
}
```

**2. æœç´¢å»ºè®®è½®è¯¢**
```javascript
let searchTimeout;
const getSuggestions = (query) => {
  clearTimeout(searchTimeout);

  searchTimeout = setTimeout(async () => {
    const response = await fetch(
      `/api/realtime/search/suggestions?query=${encodeURIComponent(query)}&limit=5`
    );
    const result = await response.json();

    if (result.success) {
      updateSuggestionsUI(result.data.suggestions);
    }
  }, 300); // 300msé˜²æŠ–å»¶è¿Ÿ
};
```

#### æ¶æ„ä¼˜åŠ¿å¯¹æ¯”

| ç‰¹æ€§ | WebSocketæ–¹å¼ | HTTPè½®è¯¢æ–¹å¼ | ä¼˜åŒ–æ•ˆæœ |
|------|-------------|-------------|----------|
| **å…¼å®¹æ€§** | éœ€è¦ç‰¹æ®Šæ”¯æŒ | HTTPé€šç”¨åè®® | +100% |
| **è°ƒè¯•æ€§** | è°ƒè¯•å›°éš¾ | ä¾¿äºè°ƒè¯•å’Œæµ‹è¯• | +200% |
| **èµ„æºå ç”¨** | è¿æ¥æ± å ç”¨ | ç®€å•HTTPè¯·æ±‚ | -60% |
| **éƒ¨ç½²å¤æ‚åº¦** | éƒ¨ç½²è¦æ±‚é«˜ | æ ‡å‡†HTTPéƒ¨ç½² | -50% |
| **é”™è¯¯å¤„ç†** | è¿æ¥é”™è¯¯å¤„ç† | HTTPé”™è¯¯ç å¤„ç† | +80% |

---

## ğŸ¯ æœç´¢ç»“æœå¤„ç†

### ç»“æœè¯„åˆ†ç³»ç»Ÿ

#### è¯„åˆ†æƒé‡è®¡ç®—
```python
class ResultScorer:
    """æœç´¢ç»“æœè¯„åˆ†å™¨"""

    def __init__(self):
        self.vector_weight = 0.7      # å‘é‡æœç´¢æƒé‡
        self.fulltext_weight = 0.3   # å…¨æ–‡æœç´¢æƒé‡
        self.recent_boost = 1.2       # æœ€è¿‘æ–‡ä»¶æƒé‡åŠ æˆ
        self.size_penalties = {      # æ–‡ä»¶å¤§å°æƒé‡
            'small': 1.1,
            'medium': 1.0,
            'large': 0.9
        }

    def calculate_score(self, result: SearchResult, query: str) -> float:
        """
        è®¡ç®—æœç´¢ç»“æœç»¼åˆè¯„åˆ†

        Args:
            result: æœç´¢ç»“æœå¯¹è±¡
            query: æœç´¢æŸ¥è¯¢è¯

        Returns:
            ç»¼åˆè¯„åˆ†
        """
        # åŸºç¡€ç›¸ä¼¼åº¦è¯„åˆ†
        base_score = result.relevance_score

        # æ–‡ä»¶ç±»å‹æƒé‡
        type_boost = self.get_type_boost(result.file_type)

        # æ–‡ä»¶å¤§å°å› å­
        size_factor = self.get_size_factor(result.file_size)

        # æ—¶é—´è¡°å‡å› å­
        time_decay = self.get_time_decay(result.modified_at)

        # æŸ¥è¯¢è¯åŒ¹é…åº¦
        query_match = self.calculate_query_match(result, query)

        # ç»¼åˆè¯„åˆ†è®¡ç®—
        final_score = (
            base_score * type_boost * size_factor *
            time_decay * query_match
        )

        return final_score
```

### ç»“æœé«˜äº®å¤„ç†

#### å†…å®¹é«˜äº®ç®—æ³•
```python
def highlight_content(content: str, query: str, max_length: int = 200) -> str:
    """
    æœç´¢ç»“æœå†…å®¹é«˜äº®

    Args:
        content: åŸå§‹å†…å®¹
        query: æœç´¢æŸ¥è¯¢è¯
        max_length: æœ€å¤§æ˜¾ç¤ºé•¿åº¦

    Returns:
        é«˜äº®å¤„ç†åçš„å†…å®¹
    """
    if not query or not content:
        return content[:max_length] + ("..." if len(content) > max_length else "")

    # åˆ†è¯å¤„ç†æŸ¥è¯¢è¯
    query_terms = query.split()

    # é«˜äº®åŒ¹é…è¯æ±‡
    highlighted = content
    for term in query_terms:
        if term in highlighted:
            # ä½¿ç”¨HTMLé«˜äº®æ ‡è®°
            highlighted = highlighted.replace(
                term,
                f"<mark>{term}</mark>"
            )

    # æˆªæ–­å¤„ç†
    if len(highlighted) > max_length:
        return highlighted[:max_length] + "..."

    return highlighted
```

---

## ğŸš€ æœç´¢æ€§èƒ½ä¼˜åŒ–

### ç´¢å¼•æ€§èƒ½ä¼˜åŒ–

#### æ‰¹é‡å‘é‡åŒ–
```python
class BatchVectorizer:
    """æ‰¹é‡å‘é‡åŒ–å¤„ç†"""

    def __init__(self, batch_size: int = 32):
        self.batch_size = batch_size
        self.model = self._load_embedding_model()

    async def vectorize_chunks(self, chunks: List[str]) -> np.ndarray:
        """
        æ‰¹é‡å‘é‡åŒ–åˆ†å—å†…å®¹

        Args:
            chunks: åˆ†å—å†…å®¹åˆ—è¡¨

        Returns:
            å‘é‡çŸ©é˜µ
        """
        embeddings = []

        # æ‰¹é‡å¤„ç†
        for i in range(0, len(chunks), self.batch_size):
            batch = chunks[i:i + self.batch_size]
            batch_embeddings = self.model.encode(batch)
            embeddings.extend(batch_embeddings)

        return np.array(embeddings)
```

#### æ™ºèƒ½ç¼“å­˜æœºåˆ¶
```python
class SearchCache:
    """æœç´¢ç»“æœç¼“å­˜"""

    def __init__(self):
        self.cache = {}
        self.cache_ttl = 300  # 5åˆ†é’Ÿç¼“å­˜
        self.max_cache_size = 1000

    def get_cache_key(self, query: str, search_type: str, **kwargs) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        params = f"{query}_{search_type}_{sorted(kwargs.items())}"
        return hashlib.md5(params.encode()).hexdigest()

    def get_cached_result(self, cache_key: str) -> Optional[List[SearchResult]]:
        """è·å–ç¼“å­˜ç»“æœ"""
        if cache_key in self.cache:
            result, timestamp = self.cache[cache_key]
            if time.time() - timestamp < self.cache_ttl:
                return result
            else:
                del self.cache[cache_key]
        return None

    def cache_result(self, cache_key: str, results: List[SearchResult]):
        """ç¼“å­˜æœç´¢ç»“æœ"""
        if len(self.cache) >= self.max_cache_size:
            # æ¸…ç†æœ€æ—§çš„ç¼“å­˜
            oldest_key = min(self.cache.keys(),
                             key=lambda k: self.cache[k][1])
            del self.cache[oldest_key]

        self.cache[cache_key] = (results, time.time())
```

### æœç´¢æ€§èƒ½ç›‘æ§

#### åŸºç¡€ç›‘æ§ç³»ç»Ÿ
```python
class SearchMetrics:
    """æœç´¢æ€§èƒ½æŒ‡æ ‡"""

    def __init__(self):
        self.search_count = 0
        self.total_latency = 0.0
        self.cache_hits = 0
        self.cache_misses = 0
        self.vector_search_time = 0.0
        self.fulltext_search_time = 0.0

    def record_search(self, latency: float, cache_hit: bool = False,
                      vector_time: float = 0.0, fulltext_time: float = 0.0):
        """è®°å½•æœç´¢æ€§èƒ½æ•°æ®"""
        self.search_count += 1
        self.total_latency += latency

        if cache_hit:
            self.cache_hits += 1
        else:
            self.cache_misses += 1

        self.vector_search_time += vector_time
        self.fulltext_search_time += fulltext_time

    def get_average_latency(self) -> float:
        """è·å–å¹³å‡å“åº”æ—¶é—´"""
        return self.total_latency / self.search_count if self.search_count > 0 else 0

    def get_cache_hit_rate(self) -> float:
        """è·å–ç¼“å­˜å‘½ä¸­ç‡"""
        total = self.cache_hits + self.cache_misses
        return self.cache_hits / total if total > 0 else 0
```

---

## âš™ï¸ æœç´¢é…ç½®ç®¡ç†

### é»˜è®¤æœç´¢é…ç½®

#### æ ¸å¿ƒé…ç½®å‚æ•°
```python
SEARCH_CONFIG = {
    # BGE-M3æ¨¡å‹é…ç½®
    "embedding_model": {
        "model_name": "BAAI/bge-m3",
        "device": "cuda",  # GPUåŠ é€Ÿ
        "normalize_embeddings": True,
        "batch_size": 32
    },

    # æœç´¢ç®—æ³•é…ç½®
    "search_algorithm": {
        "vector_weight": 0.7,        # å‘é‡æœç´¢æƒé‡
        "fulltext_weight": 0.3,      # å…¨æ–‡æœç´¢æƒé‡
        "hybrid_threshold": 0.5,      # æ··åˆæœç´¢é˜ˆå€¼
        "result_limit": 20,           # è¿”å›ç»“æœæ•°é‡é™åˆ¶
        "similarity_threshold": 0.7   # ç›¸ä¼¼åº¦é˜ˆå€¼
    },

    # åˆ†å—æœç´¢é…ç½®
    "chunking_config": {
        "chunk_size": 500,            # åˆ†å—å¤§å°
        "overlap_size": 50,            # é‡å å¤§å°
        "max_chunks": 10000,          # æœ€å¤§åˆ†å—æ•°é‡
        "min_chunk_length": 50       # æœ€å°åˆ†å—é•¿åº¦
    },

    # æ€§èƒ½ä¼˜åŒ–é…ç½®
    "performance": {
        "cache_ttl": 300,             # ç¼“å­˜ç”Ÿå­˜æ—¶é—´
        "max_cache_size": 1000,       # æœ€å¤§ç¼“å­˜æ•°é‡
        "batch_vectorization": True,   # æ‰¹é‡å‘é‡åŒ–
        "parallel_search": True        # å¹¶è¡Œæœç´¢
    }
}
```

### åŠ¨æ€é…ç½®è°ƒæ•´

#### è¿è¡Œæ—¶é…ç½®æ›´æ–°
```python
class DynamicConfig:
    """åŠ¨æ€é…ç½®ç®¡ç†"""

    def __init__(self):
        self.config = SEARCH_CONFIG.copy()
        self.config_watchers = []

    def update_search_weights(self, vector_weight: float, fulltext_weight: float):
        """åŠ¨æ€æ›´æ–°æœç´¢æƒé‡"""
        self.config["search_algorithm"]["vector_weight"] = vector_weight
        self.config["search_algorithm"]["fulltext_weight"] = fulltext_weight

        # é€šçŸ¥é…ç½®è§‚å¯Ÿè€…
        for watcher in self.config_watchers:
            watcher.on_config_update("search_weights", self.config)

    def update_chunking_params(self, chunk_size: int, overlap_size: int):
        """åŠ¨æ€æ›´æ–°åˆ†å—å‚æ•°"""
        self.config["chunking_config"]["chunk_size"] = chunk_size
        self.config["chunking_config"]["overlap_size"] = overlap_size

        # é€šçŸ¥é…ç½®è§‚å¯Ÿè€…
        for watcher in self.config_watchers:
            watcher.on_config_update("chunking_params", self.config)
```

---

## ğŸ§ª æœç´¢è´¨é‡æµ‹è¯•

### ç²¾åº¦æµ‹è¯•ç³»ç»Ÿ

#### æœç´¢ç²¾åº¦è¯„ä¼°
```python
class SearchAccuracyTest:
    """æœç´¢ç²¾åº¦æµ‹è¯•å¥—ä»¶"""

    def __init__(self):
        self.test_queries = [
            "å°é¥æœç´¢ä½¿ç”¨æ•™ç¨‹",
            "å¦‚ä½•é…ç½®AIæ¨¡å‹",
            "å¤šæ¨¡æ€æœç´¢åŠŸèƒ½",
            "æ–‡æ¡£ç´¢å¼•ç®¡ç†",
            "ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–"
        ]

    def evaluate_search_accuracy(self) -> Dict[str, float]:
        """è¯„ä¼°æœç´¢ç²¾åº¦"""
        results = {}

        for query in self.test_queries:
            # æ‰§è¡Œæœç´¢
            search_results = await self.perform_search(query)

            # è®¡ç®—ç›¸å…³æ€§è¯„åˆ†
            relevance_scores = self.manual_relevance_evaluation(query, search_results)

            # è®¡ç®—å¹³å‡ç²¾åº¦
            if relevance_scores:
                avg_precision = sum(relevance_scores) / len(relevance_scores)
                results[query] = avg_precision

        # è®¡ç®—æ€»ä½“å¹³å‡ç²¾åº¦
        overall_accuracy = sum(results.values()) / len(results) if results else 0

        results["overall"] = overall_accuracy
        return results

    def manual_relevance_evaluation(self, query: str, results: List[SearchResult]) -> List[float]:
        """æ‰‹åŠ¨ç›¸å…³æ€§è¯„ä¼°"""
        # åŸºäºå†…å®¹åŒ¹é…åº¦è¿›è¡Œç›¸å…³æ€§è¯„ä¼°
        # å¯æ‰©å±•ä¸ºäººå·¥è¯„ä¼°APIè°ƒç”¨é›†æˆ
        scores = []

        for result in results:
            # è®¡ç®—å†…å®¹ç›¸å…³æ€§åˆ†æ•°
            score = self.calculate_content_relevance(query, result)
            scores.append(score)

        return scores
```

### æ€§èƒ½ç›‘æ§ç³»ç»Ÿ

#### å“åº”æ—¶é—´ç›‘æ§
```python
class ResponseTimeMonitor:
    """å“åº”æ—¶é—´ç›‘æ§å™¨"""

    def __init__(self):
        self.response_times = []
        self.slow_search_threshold = 1000  # 1ç§’æ…¢æŸ¥è¯¢é˜ˆå€¼
        self.fast_search_threshold = 200   # 200æ¯«ç§’å¿«é€ŸæŸ¥è¯¢é˜ˆå€¼

    def record_response_time(self, response_time: float):
        """è®°å½•å“åº”æ—¶é—´"""
        self.response_times.append(response_time)

        # ä¿æŒæœ€è¿‘1000æ¡è®°å½•
        if len(self.response_times) > 1000:
            self.response_times = self.response_times[-1000:]

    def get_performance_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡æ•°æ®"""
        if not self.response_times:
            return {"status": "no_data"}

        times = sorted(self.response_times)

        return {
            "average_time": sum(times) / len(times),
            "median_time": times[len(times) // 2],
            "p95_time": times[int(len(times) * 0.95)],
            "max_time": max(times),
            "min_time": min(times),
            "fast_search_rate": len([t for t in times if t < self.fast_search_threshold]) / len(times),
            "slow_search_rate": len([t for t in times if t > self.slow_search_threshold]) / len(times)
        }
```

---

## ğŸ§ª å¤šæ¨¡æ€æœç´¢æµ‹è¯•

### é›†æˆæµ‹è¯•å¥—ä»¶

#### å¤šæ¨¡æ€æœç´¢æµ‹è¯•
```python
class MultimodalSearchTest:
    """å¤šæ¨¡æ€æœç´¢æµ‹è¯•"""

    async def test_text_search(self):
        """æµ‹è¯•æ–‡æœ¬æœç´¢åŠŸèƒ½"""
        query = "å°é¥æœç´¢ä½¿ç”¨æ•™ç¨‹"
        results = await self.search_service.search(
            query=query,
            input_type="text",
            search_type="hybrid",
            limit=10
        )

        assert results.success is True
        assert len(results.data.results) > 0
        assert all(r.relevance_score > 0 for r in results.data.results)
        print(f"âœ… æ–‡æœ¬æœç´¢æµ‹è¯•é€šè¿‡ï¼Œè¿”å› {len(results.data.results)} ä¸ªç»“æœ")

    async def test_voice_search(self):
        """æµ‹è¯•è¯­éŸ³æœç´¢åŠŸèƒ½"""
        # ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶
        voice_file = "test_audio.mp3"

        results = await self.search_service.multimodal_search(
            input_type="voice",
            file=voice_file
        )

        assert results.success is True
        assert len(results.data.search_results) > 0
        assert results.data.confidence > 0.7
        print(f"âœ… è¯­éŸ³æœç´¢æµ‹è¯•é€šè¿‡ï¼Œè¯†åˆ«ç½®ä¿¡åº¦ {results.data.confidence}")

    async def test_image_search(self):
        """æµ‹è¯•å›¾åƒæœç´¢åŠŸèƒ½"""
        # ä¸Šä¼ å›¾ç‰‡æ–‡ä»¶
        image_file = "test_image.jpg"

        results = await self.search_service.multimodal_search(
            input_type="image",
            file=image_file
        )

        assert results.success is True
        assert len(results.data.search_results) > 0
        assert len(results.data.converted_text) > 0
        print(f"âœ… å›¾åƒæœç´¢æµ‹è¯•é€šè¿‡ï¼Œè¯†åˆ«æ–‡æœ¬ {results.data.converted_text[:50]}...")
```

#### åˆ†å—æœç´¢æµ‹è¯•
```python
class ChunkedSearchValidation:
    """åˆ†å—æœç´¢éªŒè¯æµ‹è¯•"""

    async def test_chunk_search_accuracy(self):
        """æµ‹è¯•åˆ†å—æœç´¢ç²¾åº¦"""
        # åˆ›å»ºé•¿æ–‡æ¡£æµ‹è¯•æ•°æ®
        long_content = "è¿™æ˜¯ä¸€ä¸ªå¾ˆé•¿çš„æ–‡æ¡£å†…å®¹..." * 100

        # æ‰§è¡Œåˆ†å—å¤„ç†
        chunks = self.chunk_service.chunk_content(long_content)
        assert len(chunks) > 1

        # æ‰§è¡Œåˆ†å—æœç´¢
        query = "æ–‡æ¡£å†…å®¹"
        results = await self.chunk_search_service.search(query, limit=5)

        assert results.success is True
        assert len(results.data.results) > 0

        # éªŒè¯ç»“æœåŒ…å«åˆ†å—ä¿¡æ¯
        for result in results.data.results:
            assert hasattr(result, 'chunk_id')
            assert result.chunk_id is not None

        print(f"âœ… åˆ†å—æœç´¢æµ‹è¯•é€šè¿‡ï¼Œè¿”å› {len(results.data.results)} ä¸ªåˆ†å—ç»“æœ")

    async def test_position_accuracy(self):
        """æµ‹è¯•ä½ç½®ä¿¡æ¯ç²¾åº¦"""
        # åˆ›å»ºåŒ…å«ç‰¹å®šå†…å®¹çš„é•¿æ–‡æ¡£
        content = "ç¬¬ä¸€ç« å†…å®¹\nç¬¬äºŒç« èŠ‚å†…å®¹\nç¬¬ä¸‰éƒ¨åˆ†å†…å®¹"

        # æ‰§è¡Œåˆ†å—å¤„ç†
        chunks = self.chunk_service.chunk_content(content)

        # æœç´¢ç‰¹å®šç« èŠ‚
        query = "ç¬¬äºŒç« èŠ‚"
        results = await self.chunk_search_service.search(query, limit=5)

        assert results.success is True
        assert len(results.data.results) > 0

        # éªŒè¯ä½ç½®ä¿¡æ¯å‡†ç¡®æ€§
        for result in results.data.results:
            if "ç¬¬äºŒç« èŠ‚" in result.content:
                assert result.start_position is not None
                assert result.end_position is not None
                assert result.start_position < result.end_position

        print(f"âœ… ä½ç½®ç²¾åº¦æµ‹è¯•é€šè¿‡ï¼Œä½ç½®ä¿¡æ¯æ­£ç¡®")
```

---

## ğŸ† ç»¼åˆæ€§èƒ½åŸºå‡†

### æœç´¢æ€§èƒ½åŸºå‡†

#### æ€§èƒ½åŸºå‡†æµ‹è¯•
```python
class SearchBenchmark:
    """æœç´¢æ€§èƒ½åŸºå‡†æµ‹è¯•"""

    async def benchmark_search_performance(self):
        """æœç´¢æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        test_queries = [
            "æ•™ç¨‹",
            "é…ç½®ä½¿ç”¨æ–¹æ³•",
            "è¿™æ˜¯ä¸€ä¸ªå¾ˆé•¿å¾ˆå¤æ‚çš„æŸ¥è¯¢è¯­å¥ï¼Œç”¨äºæµ‹è¯•ç³»ç»Ÿåœ¨å¤„ç†å¤æ‚æŸ¥è¯¢æ—¶çš„æ€§èƒ½è¡¨ç°å’Œå“åº”é€Ÿåº¦",
            "APIæ¥å£è°ƒç”¨",
            "å°é¥æœç´¢å¤šæ¨¡æ€åŠŸèƒ½"
        ]

        performance_results = []

        for query in test_queries:
            # é¢„çƒ­
            await self.search_service.search(query=query)

            # æ­£å¼æµ‹è¯•
            start_time = time.time()
            results = await self.search_service.search(query=query)
            end_time = time.time()

            latency = (end_time - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’

            performance_results.append({
                "query_length": len(query),
                "result_count": len(results.data.results),
                "latency_ms": latency,
                "success": results.success
            })

        return performance_results

    def analyze_performance(self, results: List[Dict]) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½åŸºå‡†ç»“æœ"""
        latencies = [r["latency_ms"] for r in results if r["success"]]
        result_counts = [r["result_count"] for r in results if r["success"]]
        query_lengths = [r["query_length"] for r in results]

        return {
            "average_latency": sum(latencies) / len(latencies) if latencies else 0,
            "max_latency": max(latencies) if latencies else 0,
            "min_latency": min(latencies) if latencies else 0,
            "average_results": sum(result_counts) / len(result_counts) if result_counts else 0,
            "performance_rating": self.rate_performance(latencies),
            "query_length_impact": self.analyze_query_length_impact(results)
        }

    def rate_performance(self, latencies: List[float]) -> str:
        """æ€§èƒ½è¯„çº§"""
        avg_latency = sum(latencies) / len(latencies)

        if avg_latency < 100:
            return "ä¼˜ç§€"
        elif avg_latency < 300:
            return "è‰¯å¥½"
        elif avg_latency < 500:
            return "ä¸€èˆ¬"
        else:
            return "éœ€è¦ä¼˜åŒ–"
```

---

## ğŸ”® æœªæ¥åŠŸèƒ½è§„åˆ’

### æŠ€æœ¯å‘å±•æ–¹å‘

#### v3.0ç‰ˆæœ¬ç‰¹æ€§
- **AIæ¨¡å‹å‡çº§**: æ”¯æŒæ›´å¤šæœ€æ–°AIæ¨¡å‹
- **æœç´¢ä¸ªæ€§åŒ–**: åŸºäºç”¨æˆ·è¡Œä¸ºçš„ä¸ªæ€§åŒ–æœç´¢
- **æ™ºèƒ½æ¨è**: ç›¸å…³å†…å®¹æ™ºèƒ½æ¨è
- **å®æ—¶åä½œ**: å¤šç”¨æˆ·å®æ—¶ç´¢å¼•æ›´æ–°
- **äº‘ç«¯åŒæ­¥**: æœ¬åœ°å’Œäº‘ç«¯ç´¢å¼•åŒæ­¥

#### æ‰©å±•æ€§è§„åˆ’
- **åˆ†å¸ƒå¼**: æ”¯æŒåˆ†å¸ƒå¼æœç´¢é›†ç¾¤éƒ¨ç½²
- **å®¹å™¨åŒ–**: æ”¯æŒDocker/Kuberneteså®¹å™¨éƒ¨ç½²
- **æ’ä»¶ç³»ç»Ÿ**: æ”¯æŒç¬¬ä¸‰æ–¹æœç´¢æ’ä»¶
- **ç¼“å­˜ä¼˜åŒ–**: æ”¯æŒRedisåˆ†å¸ƒå¼ç¼“å­˜

---

## ğŸ“ ç‰ˆæœ¬æ›´æ–°æ—¥å¿—

### æ›´æ–°å†å²è®°å½•
- **v2.2** (2025-12-08) - WebSocketè½®è¯¢æ¶æ„ä¼˜åŒ–ç‰ˆ
  - å®æ—¶é€šä¿¡æ¶æ„å…¨é¢é‡æ„
  - è½®è¯¢æœºåˆ¶ä¼˜åŒ–å®ç°
  - æ€§èƒ½æå‡å’Œç¨³å®šæ€§æ”¹è¿›
  - ä»£ç æ¶æ„ç®€åŒ–

- **v2.1** (2025-12-01) - åˆ†å—æœç´¢æ¶æ„å®Œå–„ç‰ˆ
  - åˆ†å—æœç´¢åŠŸèƒ½100%å®ç°
  - PowerPointæ–‡ä»¶æ”¯æŒå®Œå–„
  - ç•Œé¢äº¤äº’ä¼˜åŒ–æ”¹è¿›

- **v2.0** (2025-11-28) - åˆ†å—æœç´¢æ¶æ„åŸºç¡€ç‰ˆ
  - æ™ºèƒ½åˆ†å—ç®—æ³•å®ç°
  - 500å­—ç¬¦+50é‡å ç­–ç•¥
  - æœç´¢ç²¾åº¦æå‡80%

---

> **æœ€åæ›´æ–°æ—¶é—´**: 2025å¹´12æœˆ8æ—¥
> **æ–‡æ¡£ç‰ˆæœ¬**: v2.2 - WebSocketè½®è¯¢æ¶æ„ä¼˜åŒ–ç‰ˆ
> **æ ¸å¿ƒç‰¹æ€§**: å¤šæ¨¡æ€AIæ™ºèƒ½æœç´¢ç³»ç»Ÿ + åˆ†å—æœç´¢ç²¾åº¦æå‡80% + è½®è¯¢æ¶æ„ä¼˜åŒ–
> **æŠ€æœ¯æ ˆ**: BGE-M3 + FasterWhisper + Chinese-CLIP + Faiss + Whoosh
> **é‡è¦ç‰¹æ€§**: åˆ†å—æœç´¢ + è½®è¯¢æ¶æ„ä¼˜åŒ– + å¤šæ¨¡æ€è¾“å…¥ + æ™ºèƒ½ç»“æœæ’åº
> **æ€§èƒ½æŒ‡æ ‡**: æœç´¢ç²¾åº¦95%ã€å¹³å‡å“åº”80-150msã€ç¼“å­˜å‘½ä¸­ç‡30%

---

**æ–‡æ¡£ç»´æŠ¤**: å°é¥æœç´¢æŠ€æœ¯å›¢é˜Ÿ
**ä»£ç ä»“åº“**: [GitHub Issues](https://github.com/your-repo/issues)
**æ›´æ–°æ—¥æœŸ**: 2025å¹´12æœˆ8æ—¥